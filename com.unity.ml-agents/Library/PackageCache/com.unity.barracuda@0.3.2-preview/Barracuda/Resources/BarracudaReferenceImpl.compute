#pragma kernel Dense
#pragma kernel Conv2D
#pragma kernel Conv2DWinograd_2x2_3x3
#pragma kernel DepthwiseConv2D
#pragma kernel Conv2DTrans
#pragma kernel Upsample2D
#pragma kernel Unstride2D
#pragma kernel MaxPool2D
#pragma kernel AvgPool2D
#pragma kernel GlobalMaxPool2D
#pragma kernel GlobalAvgPool2D
#pragma kernal GlobalAvgVariancePool2D
#pragma kernel ScaleBias
#pragma kernel InstanceNorm
#pragma kernel Dropout
#pragma kernel Relu
#pragma kernel Neg
#pragma kernel Reciprocal
#pragma kernel Swish
#pragma kernel Softmax
#pragma kernel LogSoftmax
#pragma kernel Tanh
#pragma kernel Sigmoid
#pragma kernel Relu6
#pragma kernel Elu
#pragma kernel LeakyRelu
#pragma kernel PRelu
#pragma kernel Selu
#pragma kernel Exp
#pragma kernel Log
#pragma kernel Sqrt
#pragma kernel Pow
#pragma kernel Clip
#pragma kernel Copy
#pragma kernel BroadcastAdd
#pragma kernel BroadcastSub
#pragma kernel BroadcastMul
#pragma kernel BroadcastDiv
#pragma kernel BroadcastPow
#pragma kernel BroadcastMin
#pragma kernel BroadcastMax
#pragma kernel BroadcastGreater
#pragma kernel BroadcastGreaterEqual
#pragma kernel BroadcastLess
#pragma kernel BroadcastLessEqual
#pragma kernel BroadcastEqual
#pragma kernel BroadcastLogicalOr
#pragma kernel BroadcastLogicalAnd
#pragma kernel BroadcastLogicalXor
#pragma kernel LogicalNot
#pragma kernel ReduceMin
#pragma kernel ReduceMax
#pragma kernel ReduceSum
#pragma kernel ReduceMean
#pragma kernel ReduceProd
#pragma kernel TextureToTensor
#pragma kernel TensorToTexture
#pragma kernel Border2D
#pragma kernel Pad2DEdge
#pragma kernel Pad2DReflect
#pragma kernel Pad2DSymmetric

#include "Tensor.cginc"
#include "Random.cginc"

TENSOR_DECL(X)
TENSOR_DECL(W)
TENSOR_DECL(K)
TENSOR_DECL(B)
TENSOR_DECL_RW(O)

uint4 _Pad;
uint4 _Pool;
uint4 _Stride;
float _Alpha;
float _Beta;
float _Epsilon;
float _Seed;

[numthreads(8,8,1)]
void Dense(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.flatWidth, O.flatHeight, 1);
    TENSOR_ARGS4(X, W, B, O);

    uint x = dispatchThreadID.x;
    uint y = dispatchThreadID.y;

    if (x >= O.GetFlatWidth()) return;
    if (y >= O.GetFlatHeight()) return;

    float acc = B.Get(x);
    for (uint i = 0; i < X.GetFlatWidth(); ++i)
        acc += X.Get(y, i) * W.Get(i, x);

    O.Set(y, x, acc);
}

[numthreads(4,4,4)]
void Relu(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.channels, O.width, O.height);
    TENSOR_ARGS2(X, O);

    uint c = dispatchThreadID.x;
    uint x = dispatchThreadID.y;
    uint y = dispatchThreadID.z;

    if (c >= O.channels) return;
    if (x >= O.width) return;
    if (y >= O.height) return;

    for (uint n = 0; n < X.batch; ++n)
    {
        float v = X.Get(n, y, x, c);
        v = 0.5f * (v + abs(v));

        O.Set(n, y, x, c, v);
    }
}

[numthreads(4, 4, 4)]
void PRelu(uint3 dispatchThreadID : SV_DispatchThreadID)
{
	DISPATCH_ARGS(O.channels, O.width, O.height);
	TENSOR_ARGS3(X, W, O);

	uint c = dispatchThreadID.x;
	uint x = dispatchThreadID.y;
	uint y = dispatchThreadID.z;

	if (c >= O.channels) return;
	if (x >= O.width) return;
	if (y >= O.height) return;

	for (uint n = 0; n < X.batch; ++n)
	{
		float v = X.Get(n, y, x, c);
		float slope = W.BroadcastGet(n, y, x, c);

		v = max(0.0f,v) + slope * min(0.0f,v);
		O.Set(n, y, x, c, v);
	}
}

[numthreads(4, 4, 4)]
void Selu(uint3 dispatchThreadID : SV_DispatchThreadID)
{
	DISPATCH_ARGS(O.channels, O.width, O.height);
	TENSOR_ARGS2(X, O);

	uint c = dispatchThreadID.x;
	uint x = dispatchThreadID.y;
	uint y = dispatchThreadID.z;

	if (c >= O.channels) return;
	if (x >= O.width) return;
	if (y >= O.height) return;

	for (uint n = 0; n < X.batch; ++n)
	{
		float v = X.Get(n, y, x, c);
		v = _Beta * (max(v, 0.0f) + min(_Alpha * (exp(v) - 1.0f), 0.0f));

		O.Set(n, y, x, c, v);
	}
}

[numthreads(4,4,4)]
void Neg(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.channels, O.width, O.height);
    TENSOR_ARGS2(X, O);

    uint c = dispatchThreadID.x;
    uint x = dispatchThreadID.y;
    uint y = dispatchThreadID.z;

    if (c >= O.channels) return;
    if (x >= O.width) return;
    if (y >= O.height) return;

    for (uint n = 0; n < X.batch; ++n)
    {
        float v = X.Get(n, y, x, c);
        v = -v;
        O.Set(n, y, x, c, v);
    }
}

[numthreads(4, 4, 4)]
void Reciprocal(uint3 dispatchThreadID : SV_DispatchThreadID)
{
	DISPATCH_ARGS(O.channels, O.width, O.height);
	TENSOR_ARGS2(X, O);

	uint c = dispatchThreadID.x;
	uint x = dispatchThreadID.y;
	uint y = dispatchThreadID.z;

	if (c >= O.channels) return;
	if (x >= O.width) return;
	if (y >= O.height) return;

	for (uint n = 0; n < X.batch; ++n)
	{
		float v = X.Get(n, y, x, c);
		v = 1.0f / v;
		O.Set(n, y, x, c, v);
	}
}

[numthreads(4,4,4)]
void Swish(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.channels, O.width, O.height);
    TENSOR_ARGS2(X, O);

    uint c = dispatchThreadID.x;
    uint x = dispatchThreadID.y;
    uint y = dispatchThreadID.z;

    if (c >= O.channels) return;
    if (x >= O.width) return;
    if (y >= O.height) return;

    for (uint n = 0; n < X.batch; ++n)
    {
        float v = X.Get(n, y, x, c);
        v = v / (1 + exp(-v));
        O.Set(n, y, x, c, v);
    }
}

[numthreads(4,4,4)]
void Tanh(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.channels, O.width, O.height);
    TENSOR_ARGS2(X, O);

    uint c = dispatchThreadID.x;    uint x = dispatchThreadID.y;    uint y = dispatchThreadID.z;
    if (c >= O.channels) return;    if (x >= O.width) return;       if (y >= O.height) return;

    for (uint n = 0; n < X.batch; ++n)
    {
        float v = X.Get(n, y, x, c);
        v = tanh(clamp(v,-16.0f,16.0f));//clamp to avoid NaNs for large values.

        O.Set(n, y, x, c, v);
    }
}

[numthreads(4,4,4)]
void Sigmoid(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.channels, O.width, O.height);
    TENSOR_ARGS2(X, O);

    uint c = dispatchThreadID.x;    uint x = dispatchThreadID.y;    uint y = dispatchThreadID.z;
    if (c >= O.channels) return;    if (x >= O.width) return;       if (y >= O.height) return;

    for (uint n = 0; n < X.batch; ++n)
    {
        float v = X.Get(n, y, x, c);
        v = 1 / (1 + exp(-v));
        O.Set(n, y, x, c, v);
    }
}

[numthreads(4,4,4)]
void Relu6(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.channels, O.width, O.height);
    TENSOR_ARGS2(X, O);

    uint c = dispatchThreadID.x;    uint x = dispatchThreadID.y;    uint y = dispatchThreadID.z;
    if (c >= O.channels) return;    if (x >= O.width) return;       if (y >= O.height) return;

    for (uint n = 0; n < X.batch; ++n)
    {
        float v = X.Get(n, y, x, c);
        v = min(max(0, v), 6);
        O.Set(n, y, x, c, v);
    }
}

[numthreads(4,4,4)]
void Elu(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.channels, O.width, O.height);
    TENSOR_ARGS2(X, O);

    uint c = dispatchThreadID.x;    uint x = dispatchThreadID.y;    uint y = dispatchThreadID.z;
    if (c >= O.channels) return;    if (x >= O.width) return;       if (y >= O.height) return;

    for (uint n = 0; n < X.batch; ++n)
    {
        float v = X.Get(n, y, x, c);
        if (v <= 0)
            v = _Alpha * (exp(v) - 1);
        O.Set(n, y, x, c, v);
    }
}

[numthreads(4,4,4)]
void LeakyRelu(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.channels, O.width, O.height);
    TENSOR_ARGS2(X, O);

    uint c = dispatchThreadID.x;    uint x = dispatchThreadID.y;    uint y = dispatchThreadID.z;
    if (c >= O.channels) return;    if (x >= O.width) return;       if (y >= O.height) return;

    for (uint n = 0; n < X.batch; ++n)
    {
        float v = X.Get(n, y, x, c);
        v = max(v, _Alpha * v);
        O.Set(n, y, x, c, v);
    }
}

[numthreads(4,4,4)]
void Exp(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.channels, O.width, O.height);
    TENSOR_ARGS2(X, O);

    uint c = dispatchThreadID.x;    uint x = dispatchThreadID.y;    uint y = dispatchThreadID.z;
    if (c >= O.channels) return;    if (x >= O.width) return;       if (y >= O.height) return;

    for (uint n = 0; n < X.batch; ++n)
    {
        float v = X.Get(n, y, x, c);
        v = exp(v);
        O.Set(n, y, x, c, v);
    }
}

[numthreads(4,4,4)]
void Log(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.channels, O.width, O.height);
    TENSOR_ARGS2(X, O);

    uint c = dispatchThreadID.x;    uint x = dispatchThreadID.y;    uint y = dispatchThreadID.z;
    if (c >= O.channels) return;    if (x >= O.width) return;       if (y >= O.height) return;

    for (uint n = 0; n < X.batch; ++n)
    {
        float v = X.Get(n, y, x, c);
        v = log(v);
        O.Set(n, y, x, c, v);
    }
}

[numthreads(4, 4, 4)]
void Sqrt(uint3 dispatchThreadID : SV_DispatchThreadID)
{
	DISPATCH_ARGS(O.channels, O.width, O.height);
	TENSOR_ARGS2(X, O);

	uint c = dispatchThreadID.x;    uint x = dispatchThreadID.y;    uint y = dispatchThreadID.z;
	if (c >= O.channels) return;    if (x >= O.width) return;       if (y >= O.height) return;

	for (uint n = 0; n < X.batch; ++n)
	{
		float v = X.Get(n, y, x, c);
		v = sqrt(v);
		O.Set(n, y, x, c, v);
	}
}

float signed_pow(float f, float e)
{
    // handle negative f
    float v = pow(abs(f), e);
    float s = (e % 2 == 1) ?
        sign(f):    // exponent is odd  => sign(f) * pow(abs(f), e)
        1;            // exponent is even => pow(abs(f), e)
    return v * s;
}

[numthreads(4,4,4)]
void Pow(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.channels, O.width, O.height);
    TENSOR_ARGS2(X, O);

    uint c = dispatchThreadID.x;    uint x = dispatchThreadID.y;    uint y = dispatchThreadID.z;
    if (c >= O.channels) return;    if (x >= O.width) return;       if (y >= O.height) return;

    for (uint n = 0; n < X.batch; ++n)
    {
        float v = X.Get(n, y, x, c);
        v = signed_pow(v, _Alpha);
        O.Set(n, y, x, c, v);
    }
}

[numthreads(4, 4, 4)]
void Clip(uint3 dispatchThreadID : SV_DispatchThreadID)
{
	DISPATCH_ARGS(O.channels, O.width, O.height);
	TENSOR_ARGS2(X, O);

	uint c = dispatchThreadID.x;    uint x = dispatchThreadID.y;    uint y = dispatchThreadID.z;
	if (c >= O.channels) return;    if (x >= O.width) return;       if (y >= O.height) return;

	for (uint n = 0; n < X.batch; ++n)
	{
		float v = X.Get(n, y, x, c);
		v = clamp(v, _Alpha, _Beta);
		O.Set(n, y, x, c, v);
	}
}

[numthreads(4,4,4)]
void BroadcastAdd(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.channels, O.width, O.height);
    TENSOR_ARGS3(X, B, O);

    uint c = dispatchThreadID.x;    uint x = dispatchThreadID.y;    uint y = dispatchThreadID.z;
    if (c >= O.channels) return;    if (x >= O.width) return;       if (y >= O.height) return;

    for (uint n = 0; n < X.batch; ++n)
    {
        float v =
            X.BroadcastGet(n, y, x, c) +
            B.BroadcastGet(n, y, x, c);
        O.Set(n, y, x, c, v);
    }
}

[numthreads(4,4,4)]
void BroadcastSub(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.channels, O.width, O.height);
    TENSOR_ARGS3(X, B, O);

    uint c = dispatchThreadID.x;    uint x = dispatchThreadID.y;    uint y = dispatchThreadID.z;
    if (c >= O.channels) return;    if (x >= O.width) return;       if (y >= O.height) return;

    for (uint n = 0; n < X.batch; ++n)
    {
        float v =
            X.BroadcastGet(n, y, x, c) -
            B.BroadcastGet(n, y, x, c);
        O.Set(n, y, x, c, v);
    }
}

[numthreads(4,4,4)]
void BroadcastMul(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.channels, O.width, O.height);
    TENSOR_ARGS3(X, B, O);

    uint c = dispatchThreadID.x;    uint x = dispatchThreadID.y;    uint y = dispatchThreadID.z;
    if (c >= O.channels) return;    if (x >= O.width) return;       if (y >= O.height) return;

    for (uint n = 0; n < O.batch; ++n)
    {
        float v =
            X.BroadcastGet(n, y, x, c) *
            B.BroadcastGet(n, y, x, c);
        O.Set(n, y, x, c, v);
    }
}

[numthreads(4,4,4)]
void BroadcastDiv(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.channels, O.width, O.height);
    TENSOR_ARGS3(X, B, O);

    uint c = dispatchThreadID.x;    uint x = dispatchThreadID.y;    uint y = dispatchThreadID.z;
    if (c >= O.channels) return;    if (x >= O.width) return;       if (y >= O.height) return;

    for (uint n = 0; n < X.batch; ++n)
    {
        float v =
            X.BroadcastGet(n, y, x, c) /
            B.BroadcastGet(n, y, x, c);
        O.Set(n, y, x, c, v);
    }
}

[numthreads(4,4,4)]
void BroadcastPow(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.channels, O.width, O.height);
    TENSOR_ARGS3(X, B, O);

    uint c = dispatchThreadID.x;    uint x = dispatchThreadID.y;    uint y = dispatchThreadID.z;
    if (c >= O.channels) return;    if (x >= O.width) return;       if (y >= O.height) return;

    for (uint n = 0; n < X.batch; ++n)
    {
        float v = signed_pow(
            X.BroadcastGet(n, y, x, c),
            B.BroadcastGet(n, y, x, c));
        O.Set(n, y, x, c, v);
    }
}

[numthreads(4,4,4)]
void BroadcastMin(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.channels, O.width, O.height);
    TENSOR_ARGS3(X, B, O);

    uint c = dispatchThreadID.x;    uint x = dispatchThreadID.y;    uint y = dispatchThreadID.z;
    if (c >= O.channels) return;    if (x >= O.width) return;       if (y >= O.height) return;

    for (uint n = 0; n < X.batch; ++n)
    {
        float v = min(
            X.BroadcastGet(n, y, x, c),
            B.BroadcastGet(n, y, x, c));
        O.Set(n, y, x, c, v);
    }
}

[numthreads(4,4,4)]
void BroadcastMax(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.channels, O.width, O.height);
    TENSOR_ARGS3(X, B, O);

    uint c = dispatchThreadID.x;    uint x = dispatchThreadID.y;    uint y = dispatchThreadID.z;
    if (c >= O.channels) return;    if (x >= O.width) return;       if (y >= O.height) return;

    for (uint n = 0; n < X.batch; ++n)
    {
        float v = max(
            X.BroadcastGet(n, y, x, c),
            B.BroadcastGet(n, y, x, c));
        O.Set(n, y, x, c, v);
    }
}

[numthreads(4, 4, 4)]
void BroadcastGreater(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.channels, O.width, O.height);
    TENSOR_ARGS3(X, B, O);

    uint c = dispatchThreadID.x;
    uint x = dispatchThreadID.y;
    uint y = dispatchThreadID.z;
    if (c >= O.channels)
        return;
    if (x >= O.width)
        return;
    if (y >= O.height)
        return;

    for (uint n = 0; n < X.batch; ++n)
    {
        float a = X.BroadcastGet(n, y, x, c);
        float b = B.BroadcastGet(n, y, x, c);
        float v = (a > b) ? 1.0f : 0.0f;
        O.Set(n, y, x, c, v);
    }
}

[numthreads(4, 4, 4)]
void BroadcastGreaterEqual(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.channels, O.width, O.height);
    TENSOR_ARGS3(X, B, O);

    uint c = dispatchThreadID.x;
    uint x = dispatchThreadID.y;
    uint y = dispatchThreadID.z;
    if (c >= O.channels)
        return;
    if (x >= O.width)
        return;
    if (y >= O.height)
        return;

    for (uint n = 0; n < X.batch; ++n)
    {
        float a = X.BroadcastGet(n, y, x, c);
        float b = B.BroadcastGet(n, y, x, c);
        float v = (a >= b) ? 1.0f : 0.0f;
        O.Set(n, y, x, c, v);
    }
}

[numthreads(4, 4, 4)]
void BroadcastLess(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.channels, O.width, O.height);
    TENSOR_ARGS3(X, B, O);

    uint c = dispatchThreadID.x;
    uint x = dispatchThreadID.y;
    uint y = dispatchThreadID.z;
    if (c >= O.channels)
        return;
    if (x >= O.width)
        return;
    if (y >= O.height)
        return;

    for (uint n = 0; n < X.batch; ++n)
    {
        float a = X.BroadcastGet(n, y, x, c);
        float b = B.BroadcastGet(n, y, x, c);
        float v = (a < b) ? 1.0f : 0.0f;
        O.Set(n, y, x, c, v);
    }
}

[numthreads(4, 4, 4)]
void BroadcastLessEqual(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.channels, O.width, O.height);
    TENSOR_ARGS3(X, B, O);

    uint c = dispatchThreadID.x;
    uint x = dispatchThreadID.y;
    uint y = dispatchThreadID.z;
    if (c >= O.channels)
        return;
    if (x >= O.width)
        return;
    if (y >= O.height)
        return;

    for (uint n = 0; n < X.batch; ++n)
    {
        float a = X.BroadcastGet(n, y, x, c);
        float b = B.BroadcastGet(n, y, x, c);
        float v = (a <= b) ? 1.0f : 0.0f;
        O.Set(n, y, x, c, v);
    }
}

[numthreads(4, 4, 4)]
void BroadcastEqual(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.channels, O.width, O.height);
    TENSOR_ARGS3(X, B, O);

    uint c = dispatchThreadID.x;
    uint x = dispatchThreadID.y;
    uint y = dispatchThreadID.z;
    if (c >= O.channels)
        return;
    if (x >= O.width)
        return;
    if (y >= O.height)
        return;

    for (uint n = 0; n < X.batch; ++n)
    {
        float a = X.BroadcastGet(n, y, x, c);
        float b = B.BroadcastGet(n, y, x, c);
        float v = (a == b) ? 1.0f : 0.0f;
        O.Set(n, y, x, c, v);
    }
}

[numthreads(4, 4, 4)]
void BroadcastLogicalOr(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.channels, O.width, O.height);
    TENSOR_ARGS3(X, B, O);

    uint c = dispatchThreadID.x;
    uint x = dispatchThreadID.y;
    uint y = dispatchThreadID.z;
    if (c >= O.channels)
        return;
    if (x >= O.width)
        return;
    if (y >= O.height)
        return;

    for (uint n = 0; n < X.batch; ++n)
    {
        float a = (X.BroadcastGet(n, y, x, c) == 0.0f) ? 0.0f: 1.0f;
        float b = (B.BroadcastGet(n, y, x, c) == 0.0f) ? 0.0f: 1.0f;
        float v = ((a + b) >= 1.0f) ? 1.0f : 0.0f;
        O.Set(n, y, x, c, v);
    }
}

[numthreads(4, 4, 4)]
void BroadcastLogicalAnd(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.channels, O.width, O.height);
    TENSOR_ARGS3(X, B, O);

    uint c = dispatchThreadID.x;
    uint x = dispatchThreadID.y;
    uint y = dispatchThreadID.z;
    if (c >= O.channels)
        return;
    if (x >= O.width)
        return;
    if (y >= O.height)
        return;

    for (uint n = 0; n < X.batch; ++n)
    {
        float a = (X.BroadcastGet(n, y, x, c) == 0.0f) ? 0.0f : 1.0f;
        float b = (B.BroadcastGet(n, y, x, c) == 0.0f) ? 0.0f : 1.0f;
        float v = ((a + b) > 1.5f) ? 1.0f : 0.0f;
        O.Set(n, y, x, c, v);
    }
}

[numthreads(4, 4, 4)]
void BroadcastLogicalXor(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.channels, O.width, O.height);
    TENSOR_ARGS3(X, B, O);

    uint c = dispatchThreadID.x;
    uint x = dispatchThreadID.y;
    uint y = dispatchThreadID.z;
    if (c >= O.channels)
        return;
    if (x >= O.width)
        return;
    if (y >= O.height)
        return;

    for (uint n = 0; n < X.batch; ++n)
    {
        float a = (X.BroadcastGet(n, y, x, c) == 0.0f) ? 0.0f : 1.0f;
        float b = (B.BroadcastGet(n, y, x, c) == 0.0f) ? 0.0f : 1.0f;
        float v = ((a + b) == 1.0f) ? 1.0f : 0.0f;
        O.Set(n, y, x, c, v);
    }
}

[numthreads(4, 4, 4)]
void LogicalNot(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.channels, O.width, O.height);
    TENSOR_ARGS2(X, O);

    uint c = dispatchThreadID.x;
    uint x = dispatchThreadID.y;
    uint y = dispatchThreadID.z;

    if (c >= O.channels)
        return;
    if (x >= O.width)
        return;
    if (y >= O.height)
        return;

    for (uint n = 0; n < X.batch; ++n)
    {
        float v = (X.Get(n, y, x, c) == 0.0f) ? 1.0f : 0.0f;
        O.Set(n, y, x, c, v);
    }
}

[numthreads(4,4,1)]
void ReduceMin(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.width, O.height, 1);
    TENSOR_ARGS3(X, B, O);

    uint x = dispatchThreadID.y;    uint y = dispatchThreadID.z;
    if (x >= O.width) return;       if (y >= O.height) return;

    for (uint n = 0; n < X.batch; ++n)
    {
        float minV = FLT_MAX;
        for (uint c = 0; c < X.channels; ++c)
        {
            float v = X.Get(n, y, x, c);
            minV = min(v, minV);
        }
        O.Set(n, y, x, 0, minV);
    }
}

[numthreads(4,4,1)]
void ReduceMax(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.width, O.height, 1);
    TENSOR_ARGS3(X, B, O);

    uint x = dispatchThreadID.y;    uint y = dispatchThreadID.z;
    if (x >= O.width) return;       if (y >= O.height) return;

    for (uint n = 0; n < X.batch; ++n)
    {
        float maxV = -FLT_MAX;
        for (uint c = 0; c < X.channels; ++c)
        {
            float v = X.Get(n, y, x, c);
            maxV = max(v, maxV);
        }
        O.Set(n, y, x, 0, maxV);
    }
}

[numthreads(4,4,1)]
void ReduceSum(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.width, O.height, 1);
    TENSOR_ARGS3(X, B, O);

    uint x = dispatchThreadID.y;    uint y = dispatchThreadID.z;
    if (x >= O.width) return;       if (y >= O.height) return;

    for (uint n = 0; n < X.batch; ++n)
    {
        float v = 0;
        for (uint c = 0; c < X.channels; ++c)
            v += X.Get(n, y, x, c);
        O.Set(n, y, x, 0, v);
    }
}

[numthreads(4,4,1)]
void ReduceMean(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.width, O.height, 1);
    TENSOR_ARGS3(X, B, O);

    uint x = dispatchThreadID.y;    uint y = dispatchThreadID.z;
    if (x >= O.width) return;       if (y >= O.height) return;

    for (uint n = 0; n < X.batch; ++n)
    {
        float v = 0;
        for (uint c = 0; c < X.channels; ++c)
            v += X.Get(n, y, x, c);

        v /= X.channels;
        O.Set(n, y, x, 0, v);
    }
}

[numthreads(4,4,1)]
void ReduceProd(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.width, O.height, 1);
    TENSOR_ARGS3(X, B, O);

    uint x = dispatchThreadID.y;    uint y = dispatchThreadID.z;
    if (x >= O.width) return;       if (y >= O.height) return;

    for (uint n = 0; n < X.batch; ++n)
    {
        float v = 1;
        for (uint c = 0; c < X.channels; ++c)
            v *= X.Get(n, y, x, c);
        O.Set(n, y, x, 0, v);
    }
}

[numthreads(4,4,4)]
void Copy(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    // NOTE: dispatched over X (not O)
    DISPATCH_ARGS(X.channels, X.width, X.height);
    TENSOR_ARGS2(X, O);

    uint c = dispatchThreadID.x;    uint x = dispatchThreadID.y;    uint y = dispatchThreadID.z;
    if (c >= X.channels) return;    if (x >= X.width) return;       if (y >= X.height) return;

    for (uint n = 0; n < X.batch; ++n)
    {
        float v = X.Get(n, y, x, c);
        O.Set(n + _Pad[0], y + _Pad[1], x + _Pad[2], c + _Pad[3], v);
    }
}

[numthreads(4,4,4)]
void Dropout(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.channels, O.width, O.height);
    TENSOR_ARGS2(X, O);

    uint c = dispatchThreadID.x;    uint x = dispatchThreadID.y;    uint y = dispatchThreadID.z;
    if (c >= O.channels) return;    if (x >= O.width) return;       if (y >= O.height) return;

    for (uint n = 0; n < O.batch; ++n)
    {
        float4 seed = float4(n / O.batch, y / O.height, x / O.width, c / O.channels);
        seed = frac(seed + _Seed);

        float v = X.Get(n, y, x, c);
        v *= Bernoulli(seed, 1 - _Alpha) / (1 - _Alpha);
        O.Set(n, y, x, c, v);
    }
}

[numthreads(4,4,4)]
void ScaleBias(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.channels, O.width, O.height);
    TENSOR_ARGS4(X, W, B, O);

    uint c = dispatchThreadID.x;
    uint x = dispatchThreadID.y;
    uint y = dispatchThreadID.z;

    if (c >= O.channels) return;
    if (x >= O.width) return;
    if (y >= O.height) return;

    float scale = W.Get(0, 0, 0, c);
    float bias = B.Get(0, 0, 0, c);

    for (uint n = 0; n < X.batch; ++n)
    {
        float v = X.Get(n, y, x, c);
        v = v * scale + bias;
        O.Set(n, y, x, c, v);
    }
}

[numthreads(16,4,1)]
void Softmax(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.flatWidth, O.flatHeight, 1);
    TENSOR_ARGS2(X, O);

    uint x = dispatchThreadID.x;
    uint y = dispatchThreadID.y;

    if (x >= O.GetFlatWidth()) return;
    if (y >= O.GetFlatHeight()) return;

    float maxV = -FLT_MAX;
    uint i;
    for (i = 0; i < X.GetFlatWidth(); ++i)
    {
        float v = X.Get(y, i);
        if (v > maxV)
            maxV = v;
    }

    float acc = 0.0f;
    for (i = 0; i < X.GetFlatWidth(); ++i)
    {
        float v = X.Get(y, i);
        acc += exp(v - maxV);
    }

    float v = X.Get(y, x);
    v = exp(v - maxV) / acc;
    O.Set(y, x, v);
}

[numthreads(16, 4, 1)]
void LogSoftmax(uint3 dispatchThreadID : SV_DispatchThreadID)
{
	DISPATCH_ARGS(O.flatWidth, O.flatHeight, 1);
	TENSOR_ARGS2(X, O);

	uint x = dispatchThreadID.x;
	uint y = dispatchThreadID.y;

	if (x >= O.GetFlatWidth()) return;
	if (y >= O.GetFlatHeight()) return;

	float maxV = -FLT_MAX;
	for (uint i = 0; i < X.GetFlatWidth(); ++i)
	{
		float v = X.Get(y, i);
		if (v > maxV)
			maxV = v;
	}

	float acc = 0.0f;
	for (i = 0; i < X.GetFlatWidth(); ++i)
	{
		float v = X.Get(y, i);
		acc += exp(v - maxV);
	}

	float v = X.Get(y, x);
	v = log(exp(v - maxV) / acc);
	O.Set(y, x, v);
}


[numthreads(4,4,4)]
void Upsample2D(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    // NOTE: dispatched over X (not O)
    DISPATCH_ARGS(X.channels, X.width, X.height);
    TENSOR_ARGS2(X, O);

    uint c = dispatchThreadID.x;
    uint x = dispatchThreadID.y;
    uint y = dispatchThreadID.z;

    if (c >= X.channels) return;
    if (x >= X.width) return;
    if (y >= X.height) return;

    for (uint n = 0; n < O.batch; ++n)
    {
        float v = X.Get(n, y, x, c);

        for (uint dy = 0; dy < _Pool.y; ++dy)
            for (uint dx = 0; dx < _Pool.x; ++dx)
            {
                uint oy = y * _Pool.y + dy;
                uint ox = x * _Pool.x + dx;
                O.Set(n, oy, ox, c, v);
            }
    }
}

[numthreads(4,4,4)]
void MaxPool2D(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.channels, O.width, O.height);
    TENSOR_ARGS2(X, O);

    uint c = dispatchThreadID.x;
    uint x = dispatchThreadID.y;
    uint y = dispatchThreadID.z;

    if (c >= O.channels) return;
    if (x >= O.width) return;
    if (y >= O.height) return;

    for (uint n = 0; n < X.batch; ++n)
    {
        float maxV = -FLT_MAX;
        for (uint dy = 0; dy < _Pool.y; ++dy)
            for (uint dx = 0; dx < _Pool.x; ++dx)
            {
                uint2 pos = uint2(x, y) * _Stride.xy + uint2(dx, dy);
                float v = X.SafeGet(n, pos, c, _Pad.xy, -FLT_MAX );
                maxV = max(v, maxV);
            }

        O.Set(n, y, x, c, maxV);
    }
}

[numthreads(4,4,4)]
void AvgPool2D(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.channels, O.width, O.height);
    TENSOR_ARGS2(X, O);

    uint c = dispatchThreadID.x;
    uint x = dispatchThreadID.y;
    uint y = dispatchThreadID.z;

    if (c >= O.channels) return;
    if (x >= O.width) return;
    if (y >= O.height) return;

    uint2 leftCorner = _Pad.xy;
    uint2 rightCorner = uint2(X.width, X.height) + _Pad.xy;
    for (uint n = 0; n < X.batch; ++n)
    {
        float acc = 0;
        float counter = 0;
        for (uint dy = 0; dy < _Pool.y; ++dy)
            for (uint dx = 0; dx < _Pool.x; ++dx)
            {
                uint2 pos = uint2(x, y) * _Stride.xy + uint2(dx, dy);

                bool mask = all(pos >= leftCorner) && all(pos < rightCorner);
                acc += (mask)? X.Get(n, pos.y - leftCorner.y, pos.x - leftCorner.x, c): 0;
                counter += (mask)? 1: 0;
            }

        acc /= counter;
        O.Set(n, y, x, c, acc);
    }
}

[numthreads(32,1,1)]
void GlobalMaxPool2D(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.channels, 1, 1);
    TENSOR_ARGS2(X, O);

    uint c = dispatchThreadID.x;
    if (c >= O.channels) return;
    //ASSERT(X.batch == O.batch)

    for (uint n = 0; n < X.batch; ++n)
    {
        float maxV = -FLT_MAX;
        for (uint y = 0; y < X.height; ++y)
            for (uint x = 0; x < X.width; ++x)
            {
                float v = X.Get(n, y, x, c);
                maxV = max(v, maxV);
            }

        O.Set(n, 0, 0, c, maxV);
    }
}

[numthreads(32,1,1)]
void GlobalAvgPool2D(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.channels, 1, 1);
    TENSOR_ARGS2(X, O);

    uint c = dispatchThreadID.x;
    if (c >= O.channels) return;
    //ASSERT(X.batch == O.batch)

    for (uint n = 0; n < X.batch; ++n)
    {
        float v = 0;
        for (uint y = 0; y < X.height; ++y)
            for (uint x = 0; x < X.width; ++x)
                v += X.Get(n, y, x, c);

        v /= (X.height * X.width);
        O.Set(n, 0, 0, c, v);
    }
}


[numthreads(32, 1, 1)]
void GlobalAvgVariancePool2D(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.channels, 1, 1);
    TENSOR_ARGS2(X, O);

    uint c = dispatchThreadID.x;
    if (c >= O.channels) return;
    //ASSERT(X.batch == O.batch)

    for (uint n = 0; n < X.batch; ++n)
    {
	    float mean = 0;
	    float mean2 = 0;
	    for (uint y = 0; y < X.height; ++y)
	    {
		    for (uint x = 0; x < X.width; ++x)
		    {
			    float v = X.Get(n, y, x, c);
			    mean += v;
			    mean2 += v * v;
		    }
	    }

	    mean /= (X.height * X.width);
	    mean2 /= (X.height * X.width);

	    O.Set(n, 0, 0, c, mean);
	    O.Set(n, 1, 0, c, mean2 - mean * mean);
    }
}
[numthreads(32,1,1)]
void InstanceNorm(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.channels, 1, 1);
    TENSOR_ARGS4(X, W, B, O);

    uint c = dispatchThreadID.x;
    if (c >= O.channels) return;
    //ASSERT(X.shape == O.shape)

    float gamma = W.Get(0, 0, 0, c);
    float beta = B.Get(0, 0, 0, c);

    for (uint n = 0; n < O.batch; ++n)
    {
        uint x, y;
        // calc mean
        float acc = 0;
        for (y = 0; y < O.height; ++y)
            for (x = 0; x < O.width; ++x)
                acc += X.Get(n, y, x, c);
        float mean = acc / (O.width * O.height);

        // calc variance
        acc = 0;
        for (y = 0; y < O.height; ++y)
            for (x = 0; x < O.width; ++x)
            {
                float delta = X.Get(n, y, x, c) - mean;
                acc += delta * delta;
            }
        float var = acc / (O.width * O.height);

        // normalization factor
        float invNormFactor = 1 / sqrt(var + _Epsilon);

        float scale = gamma * invNormFactor;
        float bias = beta - gamma * mean * invNormFactor;

        // apply normalization
        for (y = 0; y < O.height; ++y)
            for (x = 0; x < O.width; ++x)
            {
                float v = X.Get(n, y, x, c);
                v = v * scale + bias;
                O.Set(n, y, x, c, v);
            }
    }
}

// https://github.com/andravin/wincnn
// https://arxiv.org/pdf/1509.09308.pdf
// Winograd: 4x4 image, 3x3 kernel, 2x2 output
static const float4x4 Winograd_BT = float4x4(float4(1, 0, -1, 0), float4(0, 1, 1, 0), float4(0, -1, 1, 0), float4(0, -1, 0, 1));
static const float4x4 Winograd_B = transpose(Winograd_BT);

static const float4x3 Winograd_G = float4x3(float3(1, 0, 0), float3(0.5, 0.5, 0.5), float3(0.5, -0.5, 0.5), float3(0, 0, 1));
static const float3x4 Winograd_GT = transpose(Winograd_G);

static const float2x4 Winograd_AT = float2x4(float4(1, 1, 1, 0), float4(0, 1, -1, 1));
static const float4x2 Winograd_A = transpose(Winograd_AT);

[numthreads(64, 1, 1)]
void Conv2DWinograd_2x2_3x3(uint3 dispatchThreadID : SV_DispatchThreadID, uint3 groupThreadID: SV_GroupThreadID, uint3 groupID: SV_GroupID)
{
	DISPATCH_ARGS(K.kernelCount, O.width, O.height);
	TENSOR_ARGS4(X, K, B, O);

	uint k = dispatchThreadID.x;
	uint x = dispatchThreadID.y;
	uint y = dispatchThreadID.z;

	if (k >= K.channels) return;
	if (x >= O.width) return;
	if (y >= O.height) return;


	for (uint n = 0; n < O.batch; ++n)
	{
		float2x2 acc = B.Get(k);

		for (uint c = 0; c < X.channels; ++c)
		{
			float4x4 d = 0;

			// 16 loads per thread
			for (int ix = 0; ix < 4; ix++)
			{
				for (int iy = 0; iy < 4; iy++)
				{
					uint2 index = ((4 * groupID.yz + int2(ix, iy)) * 1 + groupThreadID.yz);
					d[ix][iy] = X.Get(n, index.yx, c);
				}
			}

			// transform kernel -- N.B: do this offline offline
			float3x3 g;
			for (int ix = 0; ix < 3; ix++)
			{
				for (int iy = 0; iy < 3; iy++)
				{
					g[ix][iy] = K.Get(ix, iy, c, k);
				}
			}
			// NB, matrix * can be written as +/-, no need for *
			float4x4 v = mul(Winograd_G,  mul(g, Winograd_GT));
			float4x4 u = mul(Winograd_BT, mul(d, Winograd_B));
			float2x2 y = mul(Winograd_AT, mul(v*u, Winograd_A));

			acc += y;
		}
		acc = transpose(acc);

		// 4 writes per thread
		for (int ix = 0; ix < 2; ix++)
		{
			for (int iy = 0; iy < 2; iy++)
			{
				uint2 index = (2 * groupID.yz + int2(ix, iy)) * 1 + groupThreadID.yz;
				O.Set(n, index.y, index.x, k, acc[ix][iy]);
			}
		}
	}
}

[numthreads(4,4,4)]
void Conv2D(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(K.kernelCount, O.width, O.height);
    TENSOR_ARGS4(X, K, B, O);

    uint k = dispatchThreadID.x;
    uint x = dispatchThreadID.y;
    uint y = dispatchThreadID.z;

    if (k >= K.channels) return;
    if (x >= O.width) return;
    if (y >= O.height) return;

    for (uint n = 0; n < O.batch; ++n)
    {
        float acc = B.Get(k);
        for (uint dy = 0; dy < K.GetKernelHeight(); ++dy)
        {
            for (uint dx = 0; dx < K.GetKernelWidth(); ++dx)
            {
                uint2 pos = uint2(x, y) * _Stride.xy + uint2(dx, dy);
                for (uint c = 0; c < X.channels; ++c)
                {
                    float v = X.SafeGet(n, pos, c, _Pad.xy);
                    acc += v * K.Get(dy, dx, c, k);
                }
            }
        }

        O.Set(n, y, x, k, acc);
    }
}

NUMTHREADS((16,4,4), (8,4,4), (4,4,4))
void DepthwiseConv2D(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(K.kernelCount, O.width, O.height);
    TENSOR_ARGS4(X, K, B, O);

    uint k = dispatchThreadID.x;
    uint x = dispatchThreadID.y;
    uint y = dispatchThreadID.z;

    if (k >= K.channels) return;
    if (x >= O.width) return;
    if (y >= O.height) return;

    for (uint n = 0; n < O.batch; ++n)
    {
        float acc = B.Get(k);
        for (uint dy = 0; dy < K.GetKernelHeight(); ++dy)
            for (uint dx = 0; dx < K.GetKernelWidth(); ++dx)
            {
                uint2 pos = uint2(x, y) * _Stride.xy + uint2(dx, dy);
                float v = X.SafeGet(n, pos, k, _Pad.xy);
                acc += v * K.Get(dy, dx, 0, k);
            }

        O.Set(n, y, x, k, acc);
    }
}

[numthreads(4,4,4)]
void Unstride2D(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.channels, O.width, O.height);
    TENSOR_ARGS2(X, O);

    uint c = dispatchThreadID.x;
    uint x = dispatchThreadID.y;
    uint y = dispatchThreadID.z;

    if (c >= O.channels) return;
    if (x >= O.width) return;
    if (y >= O.height) return;

    for (uint n = 0; n < O.batch; ++n)
    {
        int xx = (int)x - (int)_Pad.x;
        int yy = (int)y - (int)_Pad.y;

        int my = yy % _Stride.y;
        int mx = xx % _Stride.x;

        int oy = yy / _Stride.y;
        int ox = xx / _Stride.x;

        bool mask = ox >= 0 && oy >= 0 && ox < (int)X.width && oy < (int)X.height &&
            my == 0 && mx == 0;

        float v = mask ? X.Get(n, (uint)oy, (uint)ox, c) : 0;
        O.Set(n, y, x, c, v);
    }
}

[numthreads(4,4,4)]
void Conv2DTrans(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(K.kernelCount, O.width, O.height);
    TENSOR_ARGS4(X, K, B, O);

    uint k = dispatchThreadID.x;
    uint x = dispatchThreadID.y;
    uint y = dispatchThreadID.z;

    if (k >= K.channels) return;
    if (x >= O.width) return;
    if (y >= O.height) return;

    uint2 strideMask = _Stride.xy - 1;

    for (uint n = 0; n < O.batch; ++n)
    {
        float acc = B.Get(k);
        for (uint dy = (y + _Pad.y) & strideMask.y; dy < K.GetKernelHeight(); dy += _Stride.y)
        {
            for (uint dx = (x + _Pad.x) & strideMask.x; dx < K.GetKernelWidth(); dx += _Stride.x)
            {
                for (uint c = 0; c < X.channels; ++c)
                {
                    uint xx = x + dx;
                    uint yy = y + dy;

                    uint oy = (yy - _Pad.y) / _Stride.y;
                    uint ox = (xx - _Pad.x) / _Stride.x;

                    // early out if read input index fall upon leftmost outer zero padding
                    if ((xx - _Pad.x) < 0) continue;
                    if ((yy - _Pad.y) < 0) continue;

                    // early out if read input index fall upon rightmost outer zero padding
                    if (ox >= X.width) continue;
                    if (oy >= X.height) continue;

                    acc += X.Get(n, oy, ox, c) * K.Get(K.GetKernelHeight() - 1 - dy, K.GetKernelWidth() - 1 - dx, c, k);
                }
            }
        }

        O.Set(n, y, x, k, acc);
    }
}


Texture2D<float4> Xtex2D;
Texture3D<float4> Xtex3D;
Texture2DArray<float4> Xtex2DArray;
SamplerState samplerXtex2D { Filter = MIN_MAG_LINEAR_MIP_POINT; AddressU = Clamp; AddressV = Clamp; };
SamplerState samplerXtex3D { Filter = MIN_MAG_LINEAR_MIP_POINT; AddressU = Clamp; AddressV = Clamp; AddressW = Clamp; };
SamplerState samplerXtex2DArray { Filter = MIN_MAG_LINEAR_MIP_POINT; AddressU = Clamp; AddressV = Clamp; };

RWTexture2D<float4> Otex2D;
RWTexture3D<float4> Otex3D;
RWTexture2DArray<float4> Otex2DArray;

bool _FlipY;

// TODO: call TextureToTensor(v, dispatchThreadID) from Tex2DToTensor() { v = Xtex2D.SampleLevel }
[numthreads(8,8,1)]
void TextureToTensor(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    TENSOR_ARG_RW(O);

    uint b = _Pad.x;
    uint x = dispatchThreadID.x + _Pad.y;
    uint y = dispatchThreadID.y + _Pad.z;
    uint c = dispatchThreadID.z + _Pad.w;

    if (y >= O.height || x >= O.width)
        return;

    // calculate texture coordinates:
    //  offset by 0.5 to get texel centers
    //  divide by texture resolution (_Pool)
    float3 uvw = (float3)dispatchThreadID + float3(0.5f, 0.5f, 0);
    uvw /= (float3)_Pool.xyz;
    if (_FlipY)
        uvw.y = 1 - uvw.y;

    float4 v = Xtex2D.SampleLevel(samplerXtex2D, uvw.xy, 0);
    //texArray.SampleLevel(smpArray, loc, 0);

    uint4 channelWriteMask = _Stride;
    bool specialCaseWhenChannelMaskIsEmptyStoresAverage = true;
    for (int i = 0; i < 4; ++i)
    {
        if (channelWriteMask[i] == 1)
        {
            O.Set(b, y, x, c, v[i]);
            c += 1;
            specialCaseWhenChannelMaskIsEmptyStoresAverage = false;
        }
    }

    if (specialCaseWhenChannelMaskIsEmptyStoresAverage)
    {
        float avg = (v.r + v.g + v.b) / 3.0f;
        O.Set(b, y, x, c, avg);
    }
}

[numthreads(8,8,1)]
void TensorToTexture(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    TENSOR_ARG(X);

    uint b = _Pad.x;
    uint x = dispatchThreadID.x + _Pad.y;
    uint y = dispatchThreadID.y + _Pad.z;
    uint c = dispatchThreadID.z + _Pad.w;

    if (y >= X.height || x >= X.width)
        return;

    if (_FlipY)
        y = X.height - 1 - y;

    float4 v = 0;

    int channelRemainder = X.channels - c;
    if (channelRemainder == 1)
    {
        // broadcast to all channels
        v = _Alpha * X.Get(b, y, x, c) + _Beta;
    }
    else if (channelRemainder == 2)
    {
        v.r = _Alpha * X.Get(b, y, x, c+0) + _Beta;
        v.g = _Alpha * X.Get(b, y, x, c+1) + _Beta;
        v.b = 0;
        v.a = 1;
    }
    else if (channelRemainder == 3)
    {
        v.r = _Alpha * X.Get(b, y, x, c+0) + _Beta;
        v.g = _Alpha * X.Get(b, y, x, c+1) + _Beta;
        v.b = _Alpha * X.Get(b, y, x, c+2) + _Beta;
        v.a = 1;
    }
    else if (channelRemainder >= 4)
    {
        v.r = _Alpha * X.Get(b, y, x, c+0) + _Beta;
        v.g = _Alpha * X.Get(b, y, x, c+1) + _Beta;
        v.b = _Alpha * X.Get(b, y, x, c+2) + _Beta;
        v.a = _Alpha * X.Get(b, y, x, c+3) + _Beta;
    }

    Otex2D[dispatchThreadID.xy] = v;
}

[numthreads(4, 4, 4)]
void Border2D(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.channels, O.width, O.height);
    TENSOR_ARGS2(X, O);

    uint c = dispatchThreadID.x;
    uint x = dispatchThreadID.y;
    uint y = dispatchThreadID.z;

    if (c >= O.channels) return;
    if (x >= O.width) return;
    if (y >= O.height) return;

    // NOTE: negative "pad" variable crop X tensor
    int croppedWidth = _Pool.x;
    int croppedHeight = _Pool.y;
    int readX = x - _Pad.x;
    int readY = y - _Pad.y;

    for (uint n = 0; n < O.batch; ++n)
    {
        float v;
        if (readX < 0 || readX >= croppedWidth ||
            readY < 0 || readY >= croppedHeight)
        {
            v = _Beta;
        }
        else
        {
            v = X.Get(n, readY, readX, c);
        }
        O.Set(n, y, x, c, v);
    }
}

void ClampHWToTensorShape(uint4 Xshape, inout int height, inout int width)
{
    width = max(width, 0);
    height = max(height, 0);
    width = min(width, (int)Xshape.z - 1);
    height = min(height, (int)Xshape.y - 1);
}

[numthreads(4, 4, 4)]
void Pad2DEdge(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.channels, O.width, O.height);
    TENSOR_ARGS2(X, O);

    uint c = dispatchThreadID.x;
    uint x = dispatchThreadID.y;
    uint y = dispatchThreadID.z;

    if (c >= O.channels) return;
    if (x >= O.width) return;
    if (y >= O.height) return;

    int readX = x - _Pad.x;
    int readY = y - _Pad.y;
    uint4 Xshape = _Stride;

    //clamp read indices to source
    ClampHWToTensorShape(Xshape, readY, readX);

    for (uint n = 0; n < O.batch; ++n)
    {
        float v = X.Get(n, readY, readX, c);
        O.Set(n, y, x, c, v);
    }
}

[numthreads(4, 4, 4)]
void Pad2DReflect(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.channels, O.width, O.height);
    TENSOR_ARGS2(X, O);

    uint c = dispatchThreadID.x;
    uint x = dispatchThreadID.y;
    uint y = dispatchThreadID.z;

    if (c >= O.channels) return;
    if (x >= O.width) return;
    if (y >= O.height) return;

    int readX = x - _Pad.x;
    int readY = y - _Pad.y;
    uint4 Xshape = _Stride;

    int lastXIndex = Xshape.z - 1;
    int lastYIndex = Xshape.y - 1;

    //x reflect indexing
    if (readX < 0)
        readX = -readX;
    else if (readX > lastXIndex)
        readX = lastXIndex - (readX - lastXIndex);

    //y reflect indexing
    if (readY < 0)
        readY = -readY;
    else if (readY > lastYIndex)
        readY = lastYIndex - (readY - lastYIndex);

    //clamp read indices to source
    ClampHWToTensorShape(Xshape, readY, readX);

    for (uint n = 0; n < O.batch; ++n)
    {
        float v = X.Get(n, readY, readX, c);
        O.Set(n, y, x, c, v);
    }
}

[numthreads(4, 4, 4)]
void Pad2DSymmetric(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.channels, O.width, O.height);
    TENSOR_ARGS2(X, O);

    uint c = dispatchThreadID.x;
    uint x = dispatchThreadID.y;
    uint y = dispatchThreadID.z;

    if (c >= O.channels) return;
    if (x >= O.width) return;
    if (y >= O.height) return;

    int readX = x - _Pad.x;
    int readY = y - _Pad.y;
    uint4 Xshape = _Stride;

    int lastXIndex = Xshape.z - 1;
    int lastYIndex = Xshape.y - 1;

    //x symmetric indexing
    if (readX < 0)
        readX = -readX - 1;
    else if (readX > lastXIndex)
        readX = lastXIndex - (readX - lastXIndex) + 1;

    //y symmetric indexing
    if (readY < 0)
        readY = -readY - 1;
    else if (readY > lastYIndex)
        readY = lastYIndex - (readY - lastYIndex) + 1;

    //clamp read indices to source
    ClampHWToTensorShape(Xshape, readY, readX);

    for (uint n = 0; n < O.batch; ++n)
    {
        float v = X.Get(n, readY, readX, c);
        O.Set(n, y, x, c, v);
    }
}
