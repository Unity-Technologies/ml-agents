import numpy as np
from abc import ABC, abstractmethod

from mlagents.trainers.buffer import AgentBuffer
from mlagents.trainers.settings import RewardSignalSettings
from mlagents_envs.base_env import BehaviorSpec


class BaseRewardProvider(ABC):
    def __init__(self, specs: BehaviorSpec, settings: RewardSignalSettings) -> None:
        self._policy_specs = specs
        self._gamma = settings.gamma
        self._strength = settings.strength
        self._ignore_done = False

    @property
    def gamma(self) -> float:
        """
        The discount factor for the reward signal
        """
        return self._gamma

    @property
    def strength(self) -> float:
        """
        The strength multiplier of the reward provider
        """
        return self._strength

    @property
    def name(self) -> str:
        """
        The name of the reward provider. Is used for reporting and identification
        """
        class_name = self.__class__.__name__
        return class_name.replace("RewardProvider", "")

    @property
    def ignore_done(self) -> bool:
        """
        If true, when the agent is done, the rewards of the next episode must be
        used to calculate the return of the current episode.
        Is used to mitigate the positive bias in rewards with no natural end.
        """
        return self._ignore_done

    @abstractmethod
    def evaluate(self, mini_batch: AgentBuffer) -> np.ndarray:
        """
        Evaluates the reward for the data present in the Dict mini_batch. Use this when evaluating a reward
        function drawn straight from a Buffer.
        :param mini_batch: A Dict of numpy arrays (the format used by our Buffer)
            when drawing from the update buffer.
        :return: a np.ndarray of rewards generated by the reward provider
        """
        raise NotImplementedError(
            "The reward provider's evaluate method has not been implemented "
        )

    @abstractmethod
    def update(self, mini_batch: AgentBuffer) -> None:
        """
        Update the reward for the data present in the Dict mini_batch. Use this when updating a reward
        function drawn straight from a Buffer.
        :param mini_batch: A Dict of numpy arrays (the format used by our Buffer)
            when drawing from the update buffer.
        """
        raise NotImplementedError(
            "The reward provider's update method has not been implemented "
        )
